 In omics experiments , variable selection involves a large number of metabolites/ genes and a small number of samples ( the n < p problem). The ultimate goal is often the identification of one , or a few features that are different among conditions- a biomarker. Complicating biomarker identification , the p variables often contain a correlation structure due to the biology of the experiment making identifying causal compounds from correlated compounds difficult. Additionally , there may be elements in the experimental design ( blocks , batches) that introduce structure in the data. While this problem has been discussed in the literature and various strategies proposed , the over fitting problems concomitant with such approaches are rarely acknowledged. Instead of viewing a single omics experiment as a definitive test for a biomarker , an unrealistic analytical goal , we propose to view such studies as screening studies where the goal of the study is to reduce the number of features present in the second round of testing , and to limit the Type II error. Using this perspective , the performance of LASSO , ridge regression and Elastic Net was compared with the performance of an ANOVA via a simulation study and two real data comparisons. Interestingly , a dramatic increase in the number of features had no effect on Type I error for the ANOVA approach. ANOVA , even without multiple test correction , has a low false positive rates in the scenarios tested. The Elastic Net has an inflated Type I error ( from 10 to 50 %) for small numbers of features which increases with sample size. The Type II error rate for the ANOVA is comparable or lower than that for the Elastic Net leading us to conclude that an ANOVA is an effective analytical tool for the initial screening of features in omics experiments.