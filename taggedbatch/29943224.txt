 Open-ended questions , in which participants write or type their responses , are used in many areas of the behavioral sciences. Although effective in the lab , they are relatively untested in online experiments , and the quality of responses is largely unexplored. Closed-ended questions are easier to use online because they generally require only single key- or mouse-press responses and are less cognitively demanding , but they can bias the responses. We compared the data quality obtained using open and closed response formats using the continued-influence effect ( CIE) , in which participants read a series of statements about an unfolding event , one of which is unambiguously corrected later. Participants typically continue to refer to the corrected misinformation when making inferential statements about the event. We implemented this basic procedure online ( Exp. 1A , n = 78) , comparing standard open-ended responses to an alternative procedure using closed-ended responses ( Exp. 1B , n = 75). Finally , we replicated these findings in a larger preregistered study ( Exps. 2A and 2B , n = 323). We observed the CIE in all conditions: Participants continued to refer to the misinformation following a correction , and their references to the target misinformation were broadly similar in number across open- and closed-ended questions. We found that participants ' open-ended responses were relatively detailed ( including an average of 75 characters for inference questions) , and almost all responses attempted to address the question. The responses were faster , however , for closed-ended questions. Overall , we suggest that with caution it may be possible to use either method for gathering CIE data.