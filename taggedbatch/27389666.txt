 Conversational agents can not yet express empathy in nuanced ways that account for the unique circumstances of the user. Agents that possess this faculty could be used to enhance digital mental health interventions. We sought to design a conversational agent that could express empathic support in ways that might approach , or even match , human capabilities. Another aim was to assess how users might appraise such a system. Our system used a corpus-based approach to simulate expressed empathy. Responses from an existing pool of online peer support data were repurposed by the agent and presented to the user. Information retrieval techniques and word embeddings were used to select historical responses that best matched a user 's concerns. We collected ratings from 37,169 users to evaluate the system. Additionally , we conducted a controlled experiment ( N = 1284) to test whether the alleged source of a response ( human or machine) might change user perceptions. The majority of responses created by the agent ( 2986/3770 , 79.20 %) were deemed acceptable by users. However , users significantly preferred the efforts of their peers ( P < .001). This effect was maintained in a controlled study ( P = .02) , even when the only difference in responses was whether they were framed as coming from a human or a machine. Our system illustrates a novel way for machines to construct nuanced and personalized empathic utterances. However , the design had significant limitations and further research is needed to make this approach viable. Our controlled study suggests that even in ideal conditions , nonhuman agents may struggle to express empathy as well as humans. The ethical implications of empathic agents , as well as their potential iatrogenic effects , are also discussed.