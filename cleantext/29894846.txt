  Although representation learning methods developed within the framework of traditional neural networks are relatively mature , developing a spiking representation model remains a challenging problem. This paper proposes an event-based method to train a feedforward spiking neural network ( SNN) layer for extracting visual features. The method introduces a novel spike-timing-dependent plasticity ( STDP) learning rule and a threshold adjustment rule both derived from a vector quantization-like objective function subject to a sparsity constraint. The STDP rule is obtained by the gradient of a vector quantization criterion that is converted to spike-based , spatio-temporally local update rules in a spiking network of leaky , integrate-and-fire ( LIF) neurons. Independence and sparsity of the model are achieved by the threshold adjustment rule and by a softmax function implementing inhibition in the representation layer consisting of WTA-thresholded spiking neurons. Together , these mechanisms implement a form of spike-based , competitive learning. Two sets of experiments are performed on the MNIST and natural image datasets. The results demonstrate a sparse spiking visual representation model with low reconstruction loss comparable with state-of-the-art visual coding approaches , yet our rule is local in both time and space , thus biologically plausible and hardware friendly.