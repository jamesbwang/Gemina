  We explore classifier training for data sets with very few labels. We investigate this task using a neural network for nonnegative data. The network is derived from a hierarchical normalized Poisson mixture model with one observed and two hidden layers. With the single objective of likelihood optimization , both labeled and unlabeled data are naturally incorporated into learning. The neural activation and learning equations resulting from our derivation are concise and local. As a consequence , the network can be scaled using standard deep learning tools for parallelized GPU implementation. Using standard benchmarks for nonnegative data , such as text document representations , MNIST , and NIST SD19 , we study the classification performance when very few labels are used for training. In different settings , the network 's performance is compared to standard and recently suggested semisupervised classifiers. While other recent approaches are more competitive for many labels or fully labeled data sets , we find that the network studied here can be applied to numbers of few labels where no other system has been reported to operate so far.