  Previous work revealed that visual semantic information conveyed by gestures can enhance degraded speech comprehension , but the mechanisms underlying these integration processes under adverse listening conditions remain poorly understood. We used MEG to investigate how oscillatory dynamics support speech-gesture integration when integration load is manipulated by auditory ( e.g. , speech degradation) and visual semantic ( e.g. , gesture congruency) factors. Participants were presented with videos of an actress uttering an action verb in clear or degraded speech , accompanied by a matching ( mixing gesture + `` mixing '') or mismatching ( drinking gesture + `` walking '') gesture. In clear speech , alpha/beta power was more suppressed in the left inferior frontal gyrus and motor and visual cortices when integration load increased in response to mismatching versus matching gestures. In degraded speech , beta power was less suppressed over posterior STS and medial temporal lobe for mismatching compared with matching gestures , showing that integration load was lowest when speech was degraded and mismatching gestures could not be integrated and disambiguate the degraded signal. Our results thus provide novel insights on how low-frequency oscillatory modulations in different parts of the cortex support the semantic audiovisual integration of gestures in clear and degraded speech: When speech is clear , the left inferior frontal gyrus and motor and visual cortices engage because higher-level semantic information increases semantic integration load. When speech is degraded , posterior STS/middle temporal gyrus and medial temporal lobe are less engaged because integration load is lowest when visual semantic information does not aid lexical retrieval and speech and gestures can not be integrated.