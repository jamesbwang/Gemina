  In this review we address to what extent computational techniques can augment our ability to predict toxicity. The first section provides a brief history of empirical observations on toxicity dating back to the dawn of Sumerian civilization. Interestingly , the concept of dose emerged very early on , leading up to the modern emphasis on kinetic properties , which in turn encodes the insight that toxicity is not solely a property of a compound but instead depends on the interaction with the host organism. The next logical step is the current conception of evaluating drugs from a personalized medicine point of view. We review recent work on integrating what could be referred to as classical pharmacokinetic analysis with emerging systems biology approaches incorporating multiple omics data. These systems approaches employ advanced statistical analytical data processing complemented with machine learning techniques and use both pharmacokinetic and omics data. We find that such integrated approaches not only provide improved predictions of toxicity but also enable mechanistic interpretations of the molecular mechanisms underpinning toxicity and drug resistance. We conclude the chapter by discussing some of the main challenges , such as how to balance the inherent tension between the predicitive capacity of models , which in practice amounts to constraining the number of features in the models versus allowing for rich mechanistic interpretability , i.e. , equipping models with numerous molecular features. This challenge also requires patient-specific predictions on toxicity , which in turn requires proper stratification of patients as regards how they respond , with or without adverse toxic effects. In summary , the transformation of the ancient concept of dose is currently successfully operationalized using rich integrative data encoded in patient-specific models.