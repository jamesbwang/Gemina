  In its classical version , the theory of large deviations makes quantitative statements about the probability of outliers when estimating time averages , if time series data are identically independently distributed. We study large-deviation probabilities ( LDPs) for time averages in short- and long-range correlated Gaussian processes and show that long-range correlations lead to subexponential decay of LDPs. A particular deterministic intermittent map can , depending on a control parameter , also generate long-range correlated time series. We illustrate numerically , in agreement with the mathematical literature , that this type of intermittency leads to a power law decay of LDPs. The power law decay holds irrespective of whether the correlation time is finite or infinite , and hence irrespective of whether the central limit theorem applies or not.