  Many machine learning problems can be formulated as predicting labels for a pair of objects. Problems of that kind are often referred to as pairwise learning , dyadic prediction , or network inference problems. During the past decade , kernel methods have played a dominant role in pairwise learning. They still obtain a state-of-the-art predictive performance , but a theoretical analysis of their behavior has been underexplored in the machine learning literature. In this work we review and unify kernel-based algorithms that are commonly used in different pairwise learning settings , ranging from matrix filtering to zero-shot learning. To this end , we focus on closed-form efficient instantiations of Kronecker kernel ridge regression. We show that independent task kernel ridge regression , two-step kernel ridge regression , and a linear matrix filter arise naturally as a special case of Kronecker kernel ridge regression , implying that all these methods implicitly minimize a squared loss. In addition , we analyze universality , consistency , and spectral filtering properties. Our theoretical results provide valuable insights into assessing the advantages and limitations of existing pairwise learning methods.