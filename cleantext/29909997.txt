  The attractor neural network scenario is a popular scenario for memory storage in the association cortex , but there is still a large gap between models based on this scenario and experimental data. We study a recurrent network model in which both learning rules and distribution of stored patterns are inferred from distributions of visual responses for novel and familiar images in the inferior temporal cortex ( ITC). Unlike classical attractor neural network models , our model exhibits graded activity in retrieval states , with distributions of firing rates that are close to lognormal. Inferred learning rules are close to maximizing the number of stored patterns within a family of unsupervised Hebbian learning rules , suggesting that learning rules in ITC are optimized to store a large number of attractor states. Finally , we show that there exist two types of retrieval states: one in which firing rates are constant in time and another in which firing rates fluctuate chaotically.