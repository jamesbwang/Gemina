  Modern machine learning-based modeling methods are increasingly applied to clinical problems. One such application is in variable selection methods for predictive modeling. However , there is limited research comparing the performance of classic and modern for variable selection in clinical datasets. We analyzed the performance of eight different variable selection methods: four regression-based methods ( stepwise backward selection using p-value and AIC , Least Absolute Shrinkage and Selection Operator , and Elastic Net) and four tree-based methods ( Variable Selection Using Random Forest<disease> , Regularized Random Forests , Boruta , and Gradient Boosted Feature Selection). We used two clinical datasets of different sizes , a multicenter adult clinical deterioration cohort and a single center pediatric acute kidney injury cohort. Method evaluation included measures of parsimony , variable importance , and discrimination. In the large , multicenter dataset , the modern tree-based Variable Selection Using Random Forest<disease> and the Gradient Boosted Feature Selection methods achieved the best parsimony. In the smaller , single-center dataset , the classic regression-based stepwise backward selection using p-value and AIC methods achieved the best parsimony. In both datasets , variable selection tended to decrease the accuracy of the random forest<disease> models and increase the accuracy of logistic regression models. The performance of classic regression-based and modern tree-based variable selection methods is associated with the size of the clinical dataset used. Classic regression-based variable selection methods seem to achieve better parsimony in clinical prediction problems in smaller datasets while modern tree-based methods perform better in larger datasets.