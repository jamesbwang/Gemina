  Using imperfect tests may lead to biased estimates of disease frequency and measures of association. Many studies have looked into the effect of misclassification on statistical inferences. These evaluations were either within a cross-sectional study framework , assessing biased prevalence , or for cohort study designs , evaluating biased incidence rate or risk ratio estimates based on misclassification at one of the two time-points ( initial assessment or follow-up). However , both observations at risk and incident cases can be wrongly identified in longitudinal studies , leading to selection and misclassification biases , respectively. The objective of this paper was to evaluate the relative impact of selection and misclassification biases resulting from misclassification , together , on measures of incidence and risk ratio. To investigate impact on measure of disease frequency , data sets from a hypothetical cohort study with two samples collected one month apart were simulated and analyzed based on specific test and disease characteristics , with no elimination of disease during the sampling interval or clustering of observations. Direction and magnitude of bias due to selection , misclassification , and total bias was assessed for diagnostic test sensitivity and specificity ranging from 0.7 to 1.0 and 0.8 to 1.0 , respectively , and for specific disease contexts , i.e. , disease prevalences of 5 and 20 % , and disease incidences of 0.01 , 0.05 , and 0.1 cases/animal-month. A hypothetical exposure with known strength of association was also generated. A total of 1,000 cohort studies of 1,000 observations each were simulated for these six disease contexts where the same diagnostic test was used to identify observations at risk at beginning of the cohort and incident cases at its end. Our results indicated that the departure of the estimates of disease incidence and risk ratio from their true value were mainly a function of test specificity , and disease prevalence and incidence. The combination of the two biases , at baseline and follow-up , revealed the importance of a good to excellent specificity relative to sensitivity for the diagnostic test. Small divergence from perfect specificity extended quickly to disease incidence over-estimation as true prevalence increased and true incidence decreased. A highly sensitive test to exclude diseased subjects at baseline was of less importance to minimize bias than using a highly specific one at baseline. Near perfect diagnostic test attributes were even more important to obtain a measure of association close to the true risk ratio , according to specific disease characteristics , especially its prevalence. Low prevalent and high incident disease lead to minimal bias if disease is diagnosed with high sensitivity and close to perfect specificity at baseline and follow-up. For more prevalent diseases we observed large risk ratio biases towards the null value , even with near perfect diagnosis.