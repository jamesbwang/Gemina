  We study very simple sorting algorithms based on a probabilistic comparator model. In this model , errors in comparing two elements are due to ( 1) the energy or effort put in the comparison and ( 2) the difference between the compared elements. Such algorithms repeatedly compare and swap pairs of randomly chosen elements , and they correspond to natural Markovian processes. The study of these Markov chains reveals an interesting phenomenon. Namely , in several cases , the algorithm that repeatedly compares only adjacent elements is better than the one making arbitrary comparisons: in the long-run , the former algorithm produces sequences that are `` better sorted ''. The analysis of the underlying Markov chain poses interesting questions as the latter algorithm yields a nonreversible chain , and therefore its stationary distribution seems difficult to calculate explicitly. We nevertheless provide bounds on the stationary distributions and on the mixing time of these processes in several restrictions.