  In the `` speech-to-song illusion , '' certain spoken phrases are heard as highly song-like when isolated from context and repeated. This phenomenon occurs to a greater degree for some stimuli than for others , suggesting that particular cues prompt listeners to perceive a spoken phrase as song. Here we investigated the nature of these cues across four experiments. In Experiment 1 , participants were asked to rate how song-like spoken phrases were after each of eight repetitions. Initial ratings were correlated with the consistency of an underlying beat and within-syllable pitch slope , while rating change was linked to beat consistency , within-syllable pitch slope , and melodic structure. In Experiment 2 , the within-syllable pitch slope of the stimuli was manipulated , and this manipulation changed the extent to which participants heard certain stimuli as more musical than others. In Experiment 3 , the extent to which the pitch sequences of a phrase fit a computational model of melodic structure was altered , but this manipulation did not have a significant effect on musicality ratings. In Experiment 4 , the consistency of intersyllable timing was manipulated , but this manipulation did not have an effect on the change in perceived musicality after repetition. Our methods provide a new way of studying the causal role of specific acoustic features in the speech-to-song illusion via subtle acoustic manipulations of speech , and show that listeners can rapidly ( and implicitly) assess the degree to which nonmusical stimuli contain musical structure. ( PsycINFO Database Record