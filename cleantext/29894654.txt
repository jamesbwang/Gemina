  As the optical lenses for cameras always have limited depth of field , the captured images with the same scene are not all in focus. Multifocus image fusion is an efficient technology that can synthesize an all-in-focus image using several partially focused images. Previous methods have accomplished the fusion task in spatial or transform domains. However , fusion rules are always a problem in most methods. In this letter , from the aspect of focus region detection , we propose a novel multifocus image fusion method based on a fully convolutional network ( FCN) learned from synthesized multifocus images. The primary novelty of this method is that the pixel-wise focus regions are detected through a learning FCN , and the entire image , not just the image patches , are exploited to train the FCN. First , we synthesize 4500 pairs of multifocus images by repeatedly using a gaussian filter for each image from PASCAL VOC 2012 , to train the FCN. After that , a pair of source images is fed into the trained FCN , and two score maps indicating the focus property are generated. Next , an inversed score map is averaged with another score map to produce an aggregative score map , which take full advantage of focus probabilities in two score maps. We implement the fully connected conditional random field ( CRF) on the aggregative score map to accomplish and refine a binary decision map for the fusion task. Finally , we exploit the weighted strategy based on the refined decision map to produce the fused image. To demonstrate the performance of the proposed method , we compare its fused results with several start-of-the-art methods not only on a gray data set but also on a color data set. Experimental results show that the proposed method can achieve superior fusion performance in both human visual quality and objective assessment.