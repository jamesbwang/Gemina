  Scene representation-the process of converting visual sensory data into concise descriptions-is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large , labeled datasets. However , removing the reliance on human labeling remains an important open problem. To this end , we introduce the Generative Query Network ( GQN) , a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints , constructs an internal representation , and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge , paving the way toward machines that autonomously learn to understand the world around them.