  This article explores a method for more accurately estimating the main effect of the system in a typical test-collection-based evaluation of information retrieval systems , thus increasing the sensitivity of system comparisons. Randomly partitioning the test document collection allows for multiple tests of a given system and topic ( replicates). Bootstrap ANOVA can use these replicates to extract system-topic interactions-something not possible without replicates-yielding a more precise value for the system effect and a narrower confidence interval around that value. Experiments using multiple TREC collections demonstrate that removing the topic-system interactions substantially reduces the confidence intervals around the system effect as well as increases the number of significant pairwise differences found. Further , the method is robust against small changes in the number of partitions used , against variability in the documents that constitute the partitions , and the measure of effectiveness used to quantify system effectiveness.